{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTORCH TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustrating some Pytorch basics by implementing a model for MRI tumor classification:\n",
    "#### 1. Custom dataset class\n",
    "#### 2. Data-loaders\n",
    "#### 3. Implementing a basic neural network model\n",
    "#### 4. Training with data-loaders\n",
    "#### 5. Basic  evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the appropriate packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure that the appropriate packages/libraries have been installed and can be imported.  I work with a package manager called Anaconda, with installation instructions here:\n",
    "#### https://docs.anaconda.com/anaconda/install/\n",
    "#### On the google collab linux instance that I had started, I installed the following packages with Anaconda from the command line:\n",
    "#### \"conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\"      ~ Installs PyTorch and torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing torch and torchvision: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional imports which will be required for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning Problem for Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given an a brain MRI image, we want to classify whether a tumor is present or not in the image.  (Data taken from the following kaggle link: https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection). "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We've structured the data in the following directories\n",
    "    pytorch_tutorial_6.862/\n",
    "        Data/\n",
    "            train/\n",
    "                no/\n",
    "                yes/\n",
    "            val/\n",
    "                no/\n",
    "                yes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images with and without tumors are in the yes and no directories respectively.  We will train a model using images in the train/ directory and then validate it on images from the val/ directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom dataset Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As quoted in the pytorch official tutorial for dataset classes and dataloaders (https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html), \"A significant amount of the effort applied to developing machine learning algorithms is related to data preparation. PyTorch provides many tools to make data loading easy and hopefully, makes your code more readable.\"\n",
    "#### In particular, it would be very nice to have some sort of object which handles grabbing data from this directory structure without having to deal with the details everytime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barebones Dataset Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        return 0 #Does nothing for now\n",
    "    def __len__(self):\n",
    "        return 0 #Do nothing for now\n",
    "    def __getitem__(self,index):\n",
    "        return 0 #Do nothing for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We typically need to write three functions (at minimum) to effectively use the Dataset class and interface it with Dataloaders:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __init__(self):\n",
    "// This function is called whenever we initialize a dataset class.  \n",
    "\n",
    "def __len__(self):\n",
    "// Returns dataset length\n",
    "\n",
    "def __getitem__(self,index):\n",
    "// Returns a dataset element given an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing our dataset class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,root,image_size = (128,128)):\n",
    "        self.root         = root                      # The root directory of the current training (or validation or testing) dataset that we want to work with\n",
    "        self.class_names  = os.listdir(self.root)     # Lists the name of folders in our root directory, which in our case, also serves as the class names\n",
    "        self.image_size   = image_size                # Image size which will be used      \n",
    "    def __len__(self):\n",
    "        return 0 #Do nothing for now\n",
    "    def __getitem__(self,index):\n",
    "        return 0 #Do nothing for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whenever we initalize a dataset with our \"TumorDataset\" class, the code in the initialize function will be called.  In this code, we specify the root directory of the current dataset we want to work with, the list of classes in that root directory (by listing the folder names), and the desired image size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_set   = 'brain_tumor_dataset/train/'     #Specify the path to the trainining root directory\n",
    "dataset_train    = TumorDataset(path_train_set)     #Create the dataset class using the path to the root directory for the training data as an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing out properties defined in the initialization function for our class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Specified root directory for our dataset class:')\n",
    "print(dataset_train.root)\n",
    "print('')\n",
    "\n",
    "print('List of class names:' )\n",
    "print(dataset_train.class_names)\n",
    "print('')\n",
    "\n",
    "print('Hard-coded Image Size:' )\n",
    "print(dataset_train.image_size)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to get the dataset length and an item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Samples in the dataset: %d\" % len(dataset_train))\n",
    "print(\"A Sample: {}\".format(dataset_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without implemented functionality, taking the length and indexing a sample in our dataset both return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the length function for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,root,image_size = (128,128)):\n",
    "        self.root         = root                      # The root directory of the current training (or validation or testing) dataset that we want to work with\n",
    "        self.class_names  = os.listdir(self.root)     # Lists the name of folders in our root directory, which in our case, also serve as the class names\n",
    "        self.image_size   = image_size                # Image size which will be used      \n",
    "    def __len__(self):\n",
    "        dataset_size = 0   #Variable which we will use to keep track of dataset length\n",
    "        \n",
    "        for cur_class in self.class_names:\n",
    "            #Recall, that each class relates to a folder which contains our images.  In this for loop, we compute the number of entries in each folder (which corresponds to \n",
    "            #the number of samples in each class) and then add it to our total running count of the dataset size.  \n",
    "            dataset_size += len(os.listdir(self.root + cur_class + '/'))\n",
    "            \n",
    "        return dataset_size\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return 0 #Do nothing for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-intializing our dataset class and returning the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train    = TumorDataset(path_train_set)     #Create the dataset class using the path to the root directory for the training data as an input\n",
    "\n",
    "print(\"Dataset length: %d\" % (len(dataset_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our data-set has 222 total samples.  Note, this length returns the total number of no samples plus the total number of yes samples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the get item function for the dataset class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to return a particular sample of the dataset (which in this case is an image) given an index.  Note, the way I've implemented this below certainly may not be the best or most efficient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick helper function which will reshape all of our images to the same size\n",
    "def scale(im, nR, nC):\n",
    "  nR0 = len(im)     # source number of rows \n",
    "  nC0 = len(im[0])  # source number of columns \n",
    "  return [[ im[int(nR0 * r / nR)][int(nC0 * c / nC)]  \n",
    "             for c in range(nC)] for r in range(nR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,root,size = (128,128)):\n",
    "        self.root         = root\n",
    "        self.class_names  = os.listdir(self.root)\n",
    "        self.size         = size\n",
    "\n",
    "    def __len__(self):\n",
    "        dataset_size = 0\n",
    "        \n",
    "        for cur_class in self.class_names:\n",
    "            dataset_size += len(os.listdir(self.root + cur_class + '/'))\n",
    "            \n",
    "        return dataset_size\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        cur_dataset_size = 0\n",
    "        class_label      = 0\n",
    "        image_index      = index\n",
    "        \n",
    "        for cur_class in self.class_names:\n",
    "            #Loop through each of our classes\n",
    "            \n",
    "            cur_path       = self.root + cur_class + '/'\n",
    "            files_in_class = os.listdir(cur_path)\n",
    "            #Generate a list of .JPG files which correspond to our samples for this particular class\n",
    "                                       \n",
    "            cur_dataset_size += len(files_in_class)\n",
    "            #We assume that our classes are ordered back to back for indexing.  So let's say (for example), that our first class 'no', \n",
    "            #has 10 samples, our second class 'yes' has 10 samples, and 'no' comes before 'yes' in ordering.  Then dataset[9] will index the \n",
    "            #10th image in the 'no' class, while dataset[10] will index the first image in the 'yes' class.  (recall, python zero indexes).\n",
    "            #Thus, by keeping track of how much of the total dataset we've traversed, with cur_dataset_size, we can check to see whether index corresponds\n",
    "            #to the current class\n",
    "            \n",
    "            if(index < cur_dataset_size):\n",
    "                #If index is less than the current data set size, then we know that index should grab an element from this class        \n",
    "                \n",
    "                image_index = index - cur_dataset_size\n",
    "                #Find which element of this class our index corresponds too\n",
    "                \n",
    "                image = np.asarray(Image.open(cur_path + files_in_class[image_index]),dtype=np.double)\n",
    "                #Grab the image and convert it to a numpy array\n",
    "                \n",
    "                if len(image.shape) == 3:\n",
    "                    image = image[:,:,0]\n",
    "                #Take just the first channel for gray-scale\n",
    "                \n",
    "                image = np.expand_dims(scale(image,self.size[0],self.size[1]),axis = 0)\n",
    "                #Reshape the image to size 1 x 128 x 128\n",
    "                \n",
    "                return (image,class_label)\n",
    "                #Return the image and its associated label as a tuple\n",
    "            \n",
    "            class_label += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the dataset class to grab and display a couple samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we programmed getitem to return a tuple, dataset[ii][0] will return the image for the iith sample in our dataset and dataset[ii][1] will return the associated label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train    = TumorDataset(path_train_set)\n",
    "print('Tumor Example')\n",
    "index = 4\n",
    "print(' image:')\n",
    "pyplot.imshow(np.squeeze(dataset_train[index][0]),cmap = 'gray')\n",
    "pyplot.show()\n",
    "print(' label: %d' % dataset_train[index][1])\n",
    "\n",
    "print('Non-Tumor Example')\n",
    "index = 180\n",
    "print(' image:')\n",
    "pyplot.imshow(np.squeeze(dataset_train[index][0]),cmap = 'gray')\n",
    "pyplot.show()\n",
    "print(' label: %d' % dataset_train[index][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incoorporating Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a dataset class, pytorch dataloaders nicely handle the logistics of looping through the dataset given a batch size, as is typical when training neural networks using stochastic gradient descent-esque techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "#Setting our desired batch size\n",
    "\n",
    "dataloader_training = torch.utils.data.DataLoader(dataset_train,batch_size = batch_size,shuffle=True)\n",
    "#Initializing our dataloader for the training dataset.  By setting the shuffle flag to True, the dataloader will loop through the entire dataset randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below, we loop through the dataset with the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, sampled_batch in enumerate(dataloader_training):\n",
    "    cur_images = sampled_batch[0] #Grab the images from this batch\n",
    "    cur_labels = sampled_batch[1] #Grab the corresponding labels from this batch\n",
    "\n",
    "    print('Batch %d' % (i_batch + 1))\n",
    "    print('Batch Dimensions:    ', end = '')\n",
    "    print(cur_images.shape)\n",
    "    print('Batch Labels:        ', end = '')\n",
    "    print(cur_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In our dataset class, we didn't manually convert our loaded images to PyTorch tensors nor did we add any functionality to load the data by batches across the entire dataset.  Dataloaders handle both these issues for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create the relatively simple neural network model:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input (B x 1 x 128 x 128) -> conv1 -> relu \n",
    "                          -> maxpool \n",
    "                          -> conv2 -> relu \n",
    "                          -> maxpool \n",
    "                          -> fully_connected1 -> relu \n",
    "                          -> fully_connected2 -> relu \n",
    "                          -> fully_connected3 -> sigmoid -> classification output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barebones Neural Network Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorClassificationModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init should initialize what is needed for the class and the forward function operates on whatever pytorch tensor is passed to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing init and forward for our neural network class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorClassificationModel(torch.nn.Module):\n",
    "    def __init__(self,kernel_size = 5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.maxpool    = torch.nn.MaxPool2d(2,2)\n",
    "            \n",
    "        self.convlayer1      = torch.nn.Conv2d(in_channels=1,out_channels=6,kernel_size=kernel_size)\n",
    "        self.convlayer2      = torch.nn.Conv2d(in_channels=6,out_channels=15,kernel_size=kernel_size)\n",
    "        \n",
    "        self.fully_connected1 = torch.nn.Linear(15 * 29 * 29, 120)\n",
    "        self.fully_connected2 = torch.nn.Linear(120,60)\n",
    "        self.fully_connected3 = torch.nn.Linear(60,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        print('Input shape: {}'.format(x.shape))\n",
    "        \n",
    "        x = self.maxpool(torch.nn.functional.relu(self.convlayer1(x)))\n",
    "        print('After conv + pool 1: {}'.format(x.shape))\n",
    "        \n",
    "        x = self.maxpool(torch.nn.functional.relu(self.convlayer2(x)))\n",
    "        print('After conv + pool 2: {}'.format(x.shape))\n",
    "        \n",
    "        x = x.view(-1,15*29*29)\n",
    "        print('After reshaping for fully_connected layers: {}'.format(x.shape))\n",
    "        \n",
    "        x   = torch.nn.functional.relu(self.fully_connected1(x))\n",
    "        print('After fully_connected 1: {}'.format(x.shape))\n",
    "        \n",
    "        x   = torch.nn.functional.relu(self.fully_connected2(x))\n",
    "        print('After fully_connected 2: {}'.format(x.shape))\n",
    "        \n",
    "        out = torch.sigmoid(self.fully_connected3(x))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a test model and passing a pytorch tensor through the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = TumorClassificationModel().double()\n",
    "#Initializing our model\n",
    "\n",
    "dataloader_iterator = iter(dataloader_training)\n",
    "test_input, _       = next(dataloader_iterator)\n",
    "#Grabbing a single batch\n",
    "\n",
    "print('Output Shape: {}'.format(test_model(test_input).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefining the model class without the print statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorClassificationModel(torch.nn.Module):\n",
    "    def __init__(self,kernel_size = 5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.maxpool    = torch.nn.MaxPool2d(2,2)\n",
    "            \n",
    "        self.convlayer1      = torch.nn.Conv2d(in_channels=1,out_channels=6,kernel_size=kernel_size)\n",
    "        self.convlayer2      = torch.nn.Conv2d(in_channels=6,out_channels=15,kernel_size=kernel_size)\n",
    "        \n",
    "        self.fully_connected1 = torch.nn.Linear(15 * 29 * 29, 120)\n",
    "        self.fully_connected2 = torch.nn.Linear(120,60)\n",
    "        self.fully_connected3 = torch.nn.Linear(60,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.maxpool(torch.nn.functional.relu(self.convlayer1(x)))\n",
    "        x = self.maxpool(torch.nn.functional.relu(self.convlayer2(x)))\n",
    "        \n",
    "        x   = torch.nn.functional.relu(self.fully_connected1(x.view(-1,15*29*29)))\n",
    "        x   = torch.nn.functional.relu(self.fully_connected2(x))\n",
    "        out = torch.sigmoid(self.fully_connected3(x))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Classification Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing some parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs     = 4           #Number of times we loop through the entire dataset\n",
    "learning_rate  = .00001      #Optimizer step size\n",
    "batch          = 10          #Batch-size when training\n",
    "\n",
    "USE_GPU_FLAG   = 0           #If we have a GPU, using the gpu size\n",
    "    \n",
    "model = TumorClassificationModel().double()   #model we want to train\n",
    "\n",
    "if(USE_GPU_FLAG):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loss function and optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch provides a number of different options for loss functions and optimizers when training models.  Here we pick simple BCE loss and the ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function  = torch.nn.BCELoss()\n",
    "optimizer      = torch.optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss   = np.zeros(num_epochs)\n",
    "\n",
    "print('~~~~~~~~~~~~~~~~~')\n",
    "print('Starting Training')\n",
    "print('~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch %d\" % (epoch + 1))\n",
    "    \n",
    "    #Loop through the dataset in batches\n",
    "    for i_batch, sampled_batch in enumerate(dataloader_training):\n",
    "        cur_images = sampled_batch[0].to(device)\n",
    "        cur_labels = torch.unsqueeze(sampled_batch[1],1).double().to(device)\n",
    "        #Grab the current labels and images and send theem to the appropriate device\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #Since Pytorch accumulates gradients, best practice to zero all of the gradients currently tracked by the optimizer \n",
    "        #before performing the next parameter up-date\n",
    "        \n",
    "        outputs = model(cur_images)\n",
    "        #Compute the models estimated outputs given the inputs\n",
    "        \n",
    "        loss    = loss_function(outputs,cur_labels)\n",
    "        #Compute the loss of the outputs and the associated inputs\n",
    "        \n",
    "        loss.backward()\n",
    "        #Compute the gradients with backprop\n",
    "        \n",
    "        optimizer.step()\n",
    "        #Update our model parameters values \n",
    "        \n",
    "        training_loss[epoch] += loss.item()\n",
    "        \n",
    "    \n",
    "    print(\"  training   loss: %.2f\" % (training_loss[epoch]))\n",
    "    #Printing training loss after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing validation and training accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up validation dataset and dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_val_set        = 'brain_tumor_dataset/val/'\n",
    "dataset_validate    = TumorDataset(path_val_set) #Set up the validat\n",
    "dataloader_validate = torch.utils.data.DataLoader(dataset_validate,batch_size = batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_validation = 0\n",
    "for i_batch, sampled_batch in enumerate(dataloader_validate):\n",
    "    cur_images = sampled_batch[0].to(device)\n",
    "    cur_labels = torch.unsqueeze(sampled_batch[1],1).double().to(device)\n",
    "    \n",
    "    classifications = model(cur_images).cpu().detach().numpy()\n",
    "    labels          = cur_labels.cpu().detach().numpy()\n",
    "    \n",
    "    classifications[classifications > .5]  = 1\n",
    "    classifications[classifications <= .5] = 0\n",
    "    \n",
    "    correct_validation += np.sum(classifications == labels) / len(dataset_validate)\n",
    "    \n",
    "correct_training = 0\n",
    "for i_batch, sampled_batch in enumerate(dataloader_training):\n",
    "    cur_images = sampled_batch[0].to(device)\n",
    "    cur_labels = torch.unsqueeze(sampled_batch[1],1).double().to(device)\n",
    "    \n",
    "    classifications = model(cur_images).cpu().detach().numpy()\n",
    "    labels          = cur_labels.cpu().detach().numpy()\n",
    "    \n",
    "    classifications[classifications > .5]  = 1\n",
    "    classifications[classifications <= .5] = 0\n",
    "    \n",
    "    correct_training += np.sum(classifications == labels) / len(dataset_train)\n",
    "    \n",
    "print('Training   Accuracy:    %.2f' % (correct_training * 100))\n",
    "print('Validation Accuracy:    %.2f' % (correct_validation * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
